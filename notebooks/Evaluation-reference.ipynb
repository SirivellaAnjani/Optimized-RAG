{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# Comprehensive Model Evaluation and Comparison\n",
    "\n",
    "This notebook compares the performance of all implemented models:\n",
    "- Baseline (LLM + RAG + Popularity)\n",
    "- Transformer (LLM + RAG + BERT-based Transformer)\n",
    "- RGCN (LLM + RAG + Relational Graph Convolutional Network)\n",
    "- NCF (LLM + RAG + Neural Collaborative Filtering)\n",
    "\n",
    "Evaluation includes:\n",
    "1. Standard recommendation metrics (HIT@K, MRR@K, NDCG@K, Recall@K)\n",
    "2. Contextual understanding evaluation\n",
    "3. Statistical analysis\n",
    "4. Visualizations and insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env-setup",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "change-dir",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from scripts.evaluator import EvaluationVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-metrics",
   "metadata": {},
   "source": [
    "## 1. Standard Metrics Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = EvaluationVisualizer(results_file=\"data/evaluation\")\n",
    "\n",
    "# Load all model results\n",
    "model_files = {\n",
    "    'Baseline': 'baseline_metrics.json',\n",
    "    'Transformer': 'trained_transformer_metrics.json',\n",
    "    'RGCN': 'rgcn_metrics.json',\n",
    "    'NCF': 'ncf_metrics.json'\n",
    "}\n",
    "\n",
    "results_data = visualizer.load_results(model_files)\n",
    "\n",
    "# Display loaded data summary\n",
    "print(\"\\nLoaded Results Summary:\")\n",
    "for model_name, metrics in results_data.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for metric, value in list(metrics.items())[:4]:  # Show first 4 metrics\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, metrics in results_data.items():\n",
    "    for metric_type in ['HIT', 'MRR', 'NDCG', 'Recall']:\n",
    "        for k in [1, 3, 5, 10]:\n",
    "            metric_name = f'{metric_type}@{k}'\n",
    "            comparison_data.append({\n",
    "                'Model': model_name,\n",
    "                'Metric': metric_type,\n",
    "                'K': k,\n",
    "                'Score': metrics.get(metric_name, 0)\n",
    "            })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Pivot for better viewing\n",
    "pivot_df = comparison_df.pivot_table(\n",
    "    index=['Metric', 'K'],\n",
    "    columns='Model',\n",
    "    values='Score'\n",
    ")\n",
    "\n",
    "print(\"Standard Metrics Comparison\")\n",
    "print(\"=\"*80)\n",
    "display(pivot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best-performers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify which model performs best for each metric@K\n",
    "best_performers = []\n",
    "\n",
    "for metric_type in ['HIT', 'MRR', 'NDCG', 'Recall']:\n",
    "    for k in [1, 3, 5, 10]:\n",
    "        metric_name = f'{metric_type}@{k}'\n",
    "        scores = {model: metrics.get(metric_name, 0) \n",
    "                 for model, metrics in results_data.items()}\n",
    "        best_model = max(scores, key=scores.get)\n",
    "        best_score = scores[best_model]\n",
    "        \n",
    "        best_performers.append({\n",
    "            'Metric': metric_name,\n",
    "            'Best Model': best_model,\n",
    "            'Score': best_score\n",
    "        })\n",
    "\n",
    "best_df = pd.DataFrame(best_performers)\n",
    "print(\"\\nBest Performing Models\")\n",
    "print(\"=\"*80)\n",
    "display(best_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-all-metrics",
   "metadata": {},
   "source": [
    "### 1.1 Visualization: All Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-all-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualizer.plot_all_metrics_comparison(k_values=[1, 3, 5, 10])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-metric-vs-k",
   "metadata": {},
   "source": [
    "### 1.2 Visualization: Metric vs K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-line-charts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how each metric improves with K\n",
    "for metric in ['HIT', 'MRR', 'NDCG', 'Recall']:\n",
    "    fig = visualizer.plot_metric_vs_k(metric_type=metric, k_values=[1, 3, 5, 10])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-holistic",
   "metadata": {},
   "source": [
    "### 1.3 Visualization: Holistic Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models across all metrics at K=10\n",
    "fig = visualizer.plot_radar_chart(k_value=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualizer.plot_heatmap(k_values=[1, 3, 5, 10])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contextual-eval",
   "metadata": {},
   "source": [
    "## 2. Contextual Understanding Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-contextual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load contextual evaluation results\n",
    "contextual_results_path = Path(\"data/evaluation/contextual_results.json\")\n",
    "\n",
    "if contextual_results_path.exists():\n",
    "    with open(contextual_results_path, 'r') as f:\n",
    "        contextual_results = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded contextual results from {len(contextual_results)} evaluators\")\n",
    "else:\n",
    "    print(\"No contextual results found. Run contextual evaluation first.\")\n",
    "    contextual_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-contextual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average success rates across all evaluators\n",
    "if contextual_results:\n",
    "    contextual_summary = []\n",
    "    \n",
    "    for evaluator_id, models in contextual_results.items():\n",
    "        for model_name, data in models.items():\n",
    "            contextual_summary.append({\n",
    "                'Evaluator': evaluator_id,\n",
    "                'Model': model_name,\n",
    "                'Success Rate': data['success_rate'],\n",
    "                'Successes': data['success_count'],\n",
    "                'Total': data['total_rated']\n",
    "            })\n",
    "    \n",
    "    contextual_df = pd.DataFrame(contextual_summary)\n",
    "    \n",
    "    print(\"Contextual Evaluation Results\")\n",
    "    print(\"=\"*80)\n",
    "    display(contextual_df)\n",
    "    \n",
    "    # Calculate average per model\n",
    "    avg_by_model = contextual_df.groupby('Model')['Success Rate'].mean().sort_values(ascending=False)\n",
    "    print(\"\\nAverage Success Rate by Model:\")\n",
    "    print(avg_by_model)\n",
    "else:\n",
    "    print(\"No contextual data to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contextual-by-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by context type (temporal, mood, audience)\n",
    "# Placeholder: Will need to load individual query results\n",
    "\n",
    "# TODO: Break down by context type\n",
    "# - Temporal shift queries\n",
    "# - Mood context queries  \n",
    "# - Audience context queries\n",
    "\n",
    "print(\"TODO: Implement context-type breakdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-contextual",
   "metadata": {},
   "outputs": [],
   "source": [
    "if contextual_results and len(contextual_df) > 0:\n",
    "    # Bar chart of success rates\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for model in contextual_df['Model'].unique():\n",
    "        model_data = contextual_df[contextual_df['Model'] == model]\n",
    "        fig.add_trace(go.Bar(\n",
    "            name=model,\n",
    "            x=model_data['Evaluator'],\n",
    "            y=model_data['Success Rate'],\n",
    "            text=[f\"{rate:.1%}\" for rate in model_data['Success Rate']],\n",
    "            textposition='outside'\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Contextual Understanding Success Rates by Evaluator',\n",
    "        xaxis_title='Evaluator',\n",
    "        yaxis_title='Success Rate',\n",
    "        barmode='group',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-analysis",
   "metadata": {},
   "source": [
    "## 3. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improvement-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement over baseline\n",
    "baseline_metrics = results_data.get('Baseline', {})\n",
    "\n",
    "improvement_data = []\n",
    "\n",
    "for model_name, metrics in results_data.items():\n",
    "    if model_name == 'Baseline':\n",
    "        continue\n",
    "    \n",
    "    for metric_type in ['HIT', 'MRR', 'NDCG', 'Recall']:\n",
    "        for k in [1, 3, 5, 10]:\n",
    "            metric_name = f'{metric_type}@{k}'\n",
    "            baseline_score = baseline_metrics.get(metric_name, 0)\n",
    "            model_score = metrics.get(metric_name, 0)\n",
    "            \n",
    "            if baseline_score > 0:\n",
    "                improvement = ((model_score - baseline_score) / baseline_score) * 100\n",
    "            else:\n",
    "                improvement = 0\n",
    "            \n",
    "            improvement_data.append({\n",
    "                'Model': model_name,\n",
    "                'Metric': metric_name,\n",
    "                'Baseline': baseline_score,\n",
    "                'Model Score': model_score,\n",
    "                'Improvement (%)': improvement\n",
    "            })\n",
    "\n",
    "improvement_df = pd.DataFrame(improvement_data)\n",
    "\n",
    "print(\"Performance Improvement Over Baseline\")\n",
    "print(\"=\"*80)\n",
    "display(improvement_df.head(20))\n",
    "\n",
    "# Average improvement per model\n",
    "avg_improvement = improvement_df.groupby('Model')['Improvement (%)'].mean().sort_values(ascending=False)\n",
    "print(\"\\nAverage Improvement Over Baseline:\")\n",
    "print(avg_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-improvements",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of average improvements\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        x=avg_improvement.index,\n",
    "        y=avg_improvement.values,\n",
    "        text=[f\"{v:.1f}%\" for v in avg_improvement.values],\n",
    "        textposition='outside'\n",
    "    )\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Average Performance Improvement Over Baseline',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Improvement (%)',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-rankings",
   "metadata": {},
   "source": [
    "## 4. Model Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank models based on average performance across all metrics\n",
    "model_rankings = []\n",
    "\n",
    "for model_name, metrics in results_data.items():\n",
    "    all_scores = list(metrics.values())\n",
    "    avg_score = np.mean(all_scores)\n",
    "    \n",
    "    model_rankings.append({\n",
    "        'Model': model_name,\n",
    "        'Average Score': avg_score,\n",
    "        'Rank': 0  # Will be filled after sorting\n",
    "    })\n",
    "\n",
    "# Sort and assign ranks\n",
    "ranking_df = pd.DataFrame(model_rankings).sort_values('Average Score', ascending=False)\n",
    "ranking_df['Rank'] = range(1, len(ranking_df) + 1)\n",
    "\n",
    "print(\"Overall Model Rankings (Based on Average Metric Scores)\")\n",
    "print(\"=\"*80)\n",
    "display(ranking_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-insights",
   "metadata": {},
   "source": [
    "## 5. Key Insights and Findings\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "**Standard Metrics:**\n",
    "- [TO BE FILLED: Which model performs best overall?]\n",
    "- [TO BE FILLED: Which metrics show the most improvement?]\n",
    "- [TO BE FILLED: Performance trends across K values]\n",
    "\n",
    "**Contextual Understanding:**\n",
    "- [TO BE FILLED: Which model handles context shifts best?]\n",
    "- [TO BE FILLED: Performance differences by context type]\n",
    "- [TO BE FILLED: Common failure patterns]\n",
    "\n",
    "**Model Comparison:**\n",
    "- [TO BE FILLED: Baseline vs trained models]\n",
    "- [TO BE FILLED: Strengths and weaknesses of each approach]\n",
    "- [TO BE FILLED: Trade-offs (accuracy vs complexity)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recommendations",
   "metadata": {},
   "source": [
    "## 6. Recommendations for Deployment\n",
    "\n",
    "### Model Selection Recommendations:\n",
    "\n",
    "**Best Overall Model:** [TO BE FILLED]\n",
    "\n",
    "**Use Cases:**\n",
    "- For accuracy: [MODEL]\n",
    "- For speed: [MODEL]\n",
    "- For contextual understanding: [MODEL]\n",
    "- For production deployment: [MODEL]\n",
    "\n",
    "**Future Improvements:**\n",
    "- [TO BE FILLED]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "### Summary\n",
    "\n",
    "This evaluation compared four different approaches to conversational movie recommendation:\n",
    "\n",
    "1. **Baseline (LLM+RAG+Popularity)**: [SUMMARY]\n",
    "2. **Transformer**: [SUMMARY]\n",
    "3. **RGCN**: [SUMMARY]\n",
    "4. **NCF**: [SUMMARY]\n",
    "\n",
    "**Main Conclusions:**\n",
    "- [TO BE FILLED]\n",
    "- [TO BE FILLED]\n",
    "- [TO BE FILLED]\n",
    "\n",
    "**Impact of External Recommenders:**\n",
    "[TO BE FILLED: Did they improve performance? By how much?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-cell",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
